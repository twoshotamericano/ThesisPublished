# Multivariate Statistical Models {#Models}

**Estimating Best Linear Predictors**

In practice, we approach the problem of estimating the best linear predictor by making assumptions in addition to the population mean and covariance being finite. These assumptions vary from model to model. One practical reason for this is we do not have access to the population mean $\mu$ and covariance matrix $\Sigma$, but rather must make estimates $\overline{X}$ and $S$ respectively using the data available.

In the subsequent chapters I want to investigate the best linear predictor of a response variable using three different Multivariate Techniques.As we shall see each approach emphasises a different aspect of the sample data. The Classical Linear Regression approach focuses on the sample mean, the Principal Component Analysis approach emphasises the sample covariance matrix and Factor Analysis is a mixture of both!

**Classical Linear Regression**

A regression function is a conditional expectation. The expectation is calculated for one variable $Y$, the response, conditional on the values for the other "explanatory" variables $Z_{1},Z_{2},...,Z_{r}$. It need not be linear but always predicts $Y$ with smallest MSE (ref?). The regression function is written $E[Y \vert Z_{1},Z_{2},...,Z_{r}]$

In the classical linear regression model, the response $Y$ is constructed by adding an error term to a mean effect. The mean effect is defined as a linear function of the explanatory variables. When fitting classical linear models, it is important to remember that the explanatory variables are considered fixed $Z$ but the parameter values in the linear sum need to be determined $[\beta_{0},\beta_{1},..,\beta_{r}]$.

```{definition, name="Classical Linear Regression Model"}
Let $Y$ be a univariate response variable whose relationship with r predictor variables $Z_{1},Z_{2},...,Z_{r}$ is under investigation. The classical Linear Regression Model is:

\begin{align} 
Y &= \beta_{0}+\beta_{1}Z_{1}+\beta_{2}Z_{2},...+\beta_{r}Z_{r}+\epsilon \\
[Response] &= [Mean]+[Error]
\end{align} 

The following additional assumptions are made. For a Random Sample of size n, we assume:

The error term \epsilon has zero expectation:
$$E[\underset{(n \times 1)}{\epsilon}]=\underset{(n \times 1)}{0}$$

The individual components of the error vector have equal variance and are mutually uncorrelated:

$$\underset{(n \times n)}{Cov(\epsilon)}=\sigma^2\underset{(n \times n)}{I}$$

The error vector is generated by a multivariate normal distribution:

$$\underset{(n \times 1)}{\epsilon} \sim \mathcal{N}_{n}(0,\sigma^2I)$$

```

We can motivate the form of the classical linear regression model with reference to the Multivariate Normal Distribution:

```{definition, name="Regression Function for Multivariate Random variables"}
Let $Y$ and $Z_{1},Z_{2},...,Z_{r}$ be generated by a Multivariate Normal Distribution, ie.:

$$\begin{bmatrix}Y\\
Z_{1}\\...\\Z_{r}\end{bmatrix} \sim \mathcal{N}_{r+1}(\mu,\Sigma)$$

We can then infer the conditional distribution of $Y$ given $Z_{1},Z_{2},...,Z_{r}$. Using the notation of the previous section:

$$Y_{\vert Z_{1},Z_{2},...,Z_{r}} \sim \mathcal{N}_{1}(\mu_{Y}+\sigma_{XY}^{´}\sigma_{ZZ}^{-1}(Z-\mu_{Z}),\sigma_{YY}-\sigma_{ZY}^{´}\sigma_{ZZ}\sigma_{ZY})$$

We can then identify the regression function as a linear function of the vector of explanatory variables $Z$ plus a constant term:

$$E[Y \vert Z_{1},Z_{2},...,Z_{r}]=\mu_{Y}+\sigma_{XY}^{´}\sigma_{ZZ}^{-1}(Z-\mu_{Z})$$

By making the following definitions:

\begin{align} 
b_{Z} &:= \Sigma_{ZZ}^{-1}\sigma_{ZY}\\
b_{0} &:= \mu_{Y}-b_{Z}^{´}\mu_{Z}
\end{align} 

We recover the mean effect from the classical linear model namely:

$$E[Y \vert Z_{1},Z_{2},...,Z_{r}]=b_{0}+b_{Z}^´Z$$

```

The model fitting procedure is visualised as follows:

 ```{definition, name="Geometrical Picture of Linear Regression"}
The expected value of the response vector is:

$$E[Y]=X\beta=\begin{bmatrix}1 & Z_{11} & Z_{12} & ...& Z_{1r}\\1& Z_{21} & Z_{22} &... & Z_{2r}\\....&....&....&....&...\\1 & Z_{n1} & Z_{n2} &....&Z_{nr}\end{bmatrix}
                  \begin{bmatrix}\beta_{0}\\\beta_{1}\\...\\\beta_{r}\end{bmatrix}$$

This can be re-written as a sum of the columns of Z:

$$E[Y]=\beta_{0}\underset{(n \times 1)}{1}+ \beta_{1}\underset{(n \times 1)}{Z_{1}}+...\beta_{r}\underset{(n \times 1)}{Z_{r}}$$

Thus the linear regression model states that the mean vector lies in a hyperplane spanned by the r+1 measurment vectors.

The error vector capture the deviance of the response vector from the hyperplane.

```

                  
