# Multivariate Statistical Models {#Models}

**Estimating Best Linear Predictors**

In practice, the models we use to estimate the best linear predictor make assumptions in addition to the population mean and covariance being finite. These assumptions vary from model to model. One practical reason for this is we do not have access to the population mean $\mu$ and covariance matrix $\Sigma$, but rather must make inferences based on sample estimates using the data available.

In the subsequent chapters I want to investigate the best linear predictor of a response variable using three different Multivariate Techniques. My general objective is to end up with models with a limited number of factors (ie. data reduction) that offer significant insights into the data (ie ease of interpretation) and are technically sound (ie. statistical model fitting and selection).

As we shall see each approach emphasises a different aspect of the sample data. The Classical Linear Regression approach focuses on the sample mean, the Principal Component Analysis approach emphasises the sample covariance matrix and Factor Analysis is a mixture of both. 

## Classical Linear Regression 

**Analytical Properties**

A regression function is a conditional expectation. The expectation is calculated for one variable $Y$, the response, conditional on the values for the other "explanatory" variables $Z_{1},Z_{2},...,Z_{r}$. It need not be linear but always predicts $Y$ with smallest MSE (ref?). The regression function is written $E[Y \vert Z_{1},Z_{2},...,Z_{r}]$.

We can motivate the form of the classical linear regression model with reference to the Multivariate Normal Distribution:

```{example, name="Regression Function for Multivariate Random variables"}
Let $Y$ and $Z_{1},Z_{2},...,Z_{r}$ be generated by a Multivariate Normal Distribution, ie.:

$$\begin{bmatrix}Y\\
Z_{1}\\...\\Z_{r}\end{bmatrix} \sim \mathcal{N}_{r+1}(\mu,\Sigma)$$

We can then infer the conditional distribution of $Y$ given $Z_{1},Z_{2},...,Z_{r}$. Using the notation of the previous section:

$$Y_{\vert Z_{1},Z_{2},...,Z_{r}} \sim \mathcal{N}_{1}(\mu_{Y}+\sigma_{XY}^{´}\sigma_{ZZ}^{-1}(Z-\mu_{Z}),\sigma_{YY}-\sigma_{ZY}^{´}\sigma_{ZZ}\sigma_{ZY})$$

We can then identify the regression function as a linear function of the vector of explanatory variables $Z$ plus a constant term:

$$E[Y \vert Z_{1},Z_{2},...,Z_{r}]=\mu_{Y}+\sigma_{XY}^{´}\sigma_{ZZ}^{-1}(Z-\mu_{Z})$$

By making the following definitions:

\begin{align} 
b_{Z} &:= \Sigma_{ZZ}^{-1}\sigma_{ZY}\\
b_{0} &:= \mu_{Y}-b_{Z}^{´}\mu_{Z}
\end{align} 

We can write the regression function in the following form:

$$E[Y \vert Z_{1},Z_{2},...,Z_{r}]=b_{0}+b_{Z}^´Z$$

$\square$

```


In the classical linear regression model, we call the linear predictor a mean effect.The response $Y$ is generated from the mean effect by adding an error term. 

```{definition, LinReg, name="Classical Linear Regression Model"}
Let $Y$ be a univariate response variable whose relationship with r predictor variables $Z_{1},Z_{2},...,Z_{r}$ is under investigation. The classical Linear Regression Model is:

\begin{align} 
Y &= \beta_{0}+\beta_{1}Z_{1}+\beta_{2}Z_{2},...+\beta_{r}Z_{r}+\epsilon \\
[Response] &= [Mean]+[Error]
\end{align} 

The following additional assumptions are made. For a Random Sample of size n, we assume:

The error term \epsilon has zero expectation:
$$E[\underset{(n \times 1)}{\epsilon}]=\underset{(n \times 1)}{0}$$

The individual components of the error vector have equal variance and are mutually uncorrelated:

$$\underset{(n \times n)}{Cov(\epsilon)}=\sigma^2\underset{(n \times n)}{I}$$

The error vector is generated by a multivariate normal distribution:

$$\underset{(n \times 1)}{\epsilon} \sim \mathcal{N}_{n}(0,\sigma^2I)$$

$\square$

```

When fitting classical linear models, it is important to remember that the explanatory variables $Z$ are considered fixed but the parameter values $[\beta_{0},\beta_{1},..,\beta_{r}]$ need to be determined . The geometrical picture of a Classical Linear Regression model is as follows:

 ```{definition, name="Geometrical Picture of Linear Regression"}
The expected value of the response vector is:

$$E[Y]=X\beta=\begin{bmatrix}1 & Z_{11} & Z_{12} & ...& Z_{1r}\\1& Z_{21} & Z_{22} &... & Z_{2r}\\....&....&....&....&...\\1 & Z_{n1} & Z_{n2} &....&Z_{nr}\end{bmatrix}
                  \begin{bmatrix}\beta_{0}\\\beta_{1}\\...\\\beta_{r}\end{bmatrix}$$

This can be re-written as a sum of the columns of Z:

$$E[Y]=\beta_{0}\underset{(n \times 1)}{1}+ \beta_{1}\underset{(n \times 1)}{Z_{1}}+...\beta_{r}\underset{(n \times 1)}{Z_{r}}$$

Thus the linear regression model states that the mean vector lies in a hyperplane spanned by the r+1 measurment vectors. These vectors are fixed by the measurement data and model fitting corresponds to finding the parameter values 
$[\beta_{0},\beta_{1},..,\beta_{r}]$ which minimise the Mean Square Error of the sample data. 

The hyperplane is known as the model plane. 

$\square$

```
**Fitting Model**

Estimating the best linear predictor (aka. the mean effect) for a Classical Linear Regression model can be achieved with ordinary least squares (OLS). Geometrically the OLS estimation procedure finds sample estimates for the parameter values $[\beta_{0},\beta_{1},..,\beta_{r}]$ which place the response vector $y$ and a vector in the model plane as close together as possible. This corresponds to decomposing the response vector $y$ in terms of a projection onto the model plane (aka. the prediction vector) and a vector of residuals orthogonal to the model plane (aka. the residual vector)

(ref:LineRegression) A representation of OLS model fitting with three observations ($n=3$) of one explanatory variable ($col_{2}(Z)$). Reproduced from [], Chapter 7

```{r LineRegression,echo=FALSE, fig.cap='(ref:LineRegression)',fig.align='center',dpi=20}
knitr::include_graphics("Analysis/Images/LinearRegression.png")
```


 ```{proposition, OLSReg, name="OLS Estimates of Linear Regression Model"}

The r+1 dimensional vector of sample estimates for the parameter values $\beta=[\beta_{0},\beta_{1},..,\beta_{r}]$ are denoted 
$\widehat{\beta}=[b_{0},b_{1},..,b_{r}]$. The OLS estiamtes of $\widehat{\beta}$ are (for proof see []):

$$\widehat{\beta}=(Z^´Z)^{-1}Z^´y$$

The n dimensional projection of the response vector onto the model plane is known as the prediction vector and denoted $\widehat{y}$. The OLS estimate is:

$$\widehat{y}=Z(Z^´Z)^{-1}Z^´y$$


The n dimensional vector of differences between the responses $y$ and the predictions $\widehat{y}$ is known as the residual vector and denoted $\widehat{\epsilon}$. The OLS estimate is:

$$\widehat{\epsilon}=(1-Z(Z^´Z)^{-1}Z^´)y$$.

$\square$

```



The ability of the best linear predictor to explain variations in the response vector is estimated using the Coefficient of Determination ($=\rho_{Y(Z)}^2$)

 ```{proposition, name="Measuring the Performance of Best Estimator"}

The orthogonality of $\widehat{\epsilon}$ and $\widehat{y}$ under OLS fitting allows the decomposition (for proof see []):

\begin{align}
\sum_{i=1}^{n}(y_{i}-\overline{y})^2 &= \sum_{i=1}^{n}(\widehat{y_{i}}-\overline{y})^2 + \sum_{i=1}^{n}(y_{i}-\widehat{y_{i}})^2 \\
[Total Sum of Squares] &= [Regression Sum of Squares]+[Residual Sum of Squares]
\end{align}

The coefficient of determination $R^2$ is calculated as follows:

\begin{align}
R^2 &= 1-\frac{\sum_{i=1}^{n}(y_{i}-\widehat{y_{i}})^2}{\sum_{i=1}^{n}(y_{i}-\overline{y})^2} \\
&=  \frac{\sum_{i=1}^{n}(\widehat{y_{i}}-\overline{y})^2}{\sum_{i=1}^{n}(y_{i}-\overline{y})^2} \\
&= [Regression Sum of Squares]/[Total Sum of Squares]
\end{align}

$\square$

```


If the model assumptions are valid then the Sample Estimates of the model parameters have the following distributional properties:

 ```{proposition, OLSSampling, name="Sampling Properties of OLS Estimates"}

The parameter estimates $\widehat{\beta}$ have the follow properties. For proof see []:

\begin{align}
E[\widehat{\beta}] &= \beta \\
Cov[\widehat{\beta}] &= \sigma^2(Z^´Z)^{-1} \\
\widehat{\beta} &\sim \mathcal{N}_{r+1}(\beta,\sigma^2(Z^´Z)^{-1}) \\
\end{align}

The sample estimates of the error term:

\begin{align}
s^2 &:= \frac{\widehat{\epsilon}^´\widehat{\epsilon}}{n-(r+1)} \\
E[s^2] &= \sigma^2 \\
\widehat{\epsilon}^´\widehat{\epsilon} &\sim \sigma^2\chi_{n-r-1}^2 
\end{align}

$\square$

```

## Principal Component Analysis

Principal Component Analysis (PCA) seeks to find an approximation to the Sample Data using fewer variables. More than Linear Regression, it relies on the information in the Sample Covariance Matrix. These transformed variables or "Principal Components" may themselves offer interesting interpretations or can become inputs in further analyses.

**Analytical Properties**

PCA analysis can be motivated by both linear algebra and geometrical arguments. Both are resproduced below:

**Linear Analysis Approach**:

PCA assumes the existence of a set of p-dimensional eigenvectors. The key result from Linear Analysis guaranteeing their existence is the Spectral Decomposition Theorem. This allows a matrix to be decomposed as a sum of matrix products of its eigenvectors and eigenvalues. It applies to symmetric square matrices:


 ```{theorem, name="Spectral Decomposition Theorem"}

Let $\underset{(p \times p)}{S}$ denote a symmetric, positive definite, non-singular matrix. Then we can decompose $S$ as follows:

$$\underset{(p \times p)}{S}= \sum_{i=1}^{p} \lambda_{i}\underset{(p \times 1)}{e_{i}}\underset{(p \times 1)}{e_{i}^´}$$

where $\lambda_{i}$ > 0 are the eigenvalues (ordered from largest to smallest) and $e_{i}$ are the orthonormal eigenvectors of S.

In matrix form this result can be re-arranged as follows:

$$\underset{(p \times p)}{S}=\underset{(p \times p)}{P}\underset{(p \times p)}{\Lambda}\underset{(p \times p)}{P^{´}}$$

where $P=[e_{1},....,e_{p}]$ and $\Lambda=diag[\lambda_{1},...,\lambda_{p}]$

$\square$

```

 ```{definition, name="Error of Approximation Sum of Squares"}

Let $\underset{(n \times p)}{A}=[a_{1},a_{2},...,a_{n}]^´$ denote an approximation to the mean corrected data matrix $$\underset{n \times p}{X}=[x_{1}-\overline{x},x_{2}-\overline{x},...,x_{n}-\overline{x}]^´$$

The error of approximation is defined as the sum of np squared errors:


$$\sum_{j=1}^{n}(x_{j}-\overline{x}-a_{j})^´(x_{j}-\overline{x}-a_{j})=
\sum_{j=1}^{n}\sum_{i=1}^{p}(x_{ji}-\overline{x_{i}}-a_{ji})^{2}$$

$\square$
```
Using linear algebra (proof not shown) we obtain the following minimisation result for the error sum of squares (p.467):

```{proposition, name="Minimisation of Error Sum of Squares"}
Let $\underset{(n \times p)}{A}$ be any matrix with $rank(A) \leq r < min(p,n)$. Let $\underset{(p \times r)}{E_{r}}$ be the matrix formed by the first r eigenvectors of $S$, ie. $\widehat{E_{r}}=[\widehat{e_{1}},\widehat{e_{2}},...,\widehat{e_{r}}]$. The error of approximation sum of squares is minimised by the selection:

$$\widehat{A}=\begin{bmatrix}(x_{1}-\overline{x})^´\\
(x_{2}-\overline{x})^´\\
\vdots \\
(x_{n}-\overline{x})^´ \end{bmatrix}\widehat{E_{r}}\widehat{E_{r}}^´=[\widehat{y_{1}},\widehat{y_{2}},..,\widehat{y_{r}}]\widehat{E_{r}^´}$$

This means the j-th observation $x_{j}-\overline{x}$(j-th column of $X^´$) is approximated as:

$$\widehat{a_{j}}=\widehat{y_{j1}}\widehat{e_{1}}+\widehat{y_{j2}}\widehat{e_{2}}+...+\widehat{y_{jr}}\widehat{e_{r}}$$
where the component $\widehat{y_{jr}}$ is the projection of $X_{j}$ onto  $\widehat{e_{r}}$. Namely:

$$[\widehat{y_{j1}},\widehat{y_{j2}},...,\widehat{y_{jr}}]^´=
[\widehat{e_{1}}^´(x_{j}-\overline{x}),\widehat{e_{2}}^´(x_{j}-\overline{x}),...,\widehat{e_{r}}^´(x_{j}-\overline{x})]^´$$

Furthermore the minimum value for the approximation sum of squares is:

$$\sum_{j=1}^{n}(x_{j}-\overline{x}-a_{j})^´(x_{j}-\overline{x}-a_{j})=
(n-1)(\widehat{\lambda_{r+1}}+...+\widehat{\lambda_{p}})$$

where $\widehat{\lambda_{r+1}},...,\widehat{\lambda_{p}}$ are the eigenvalues of the eigenvectors not included in $\widehat{E_{r}}$.


$\square$
```


In applied PCA we examine how many principal components must be added to explain a majority of the variation in the data. The algebraic solution for the minimum value of the sum of square errors motivates the following definition for measuring "total variation in the data":

 ```{definition, TotalVar, name="Total Sample Variance"}
 
 The Total Sample Variance is defined to equal:
 
 $$trace(S)= \sum_{i=1}^{p} s_{ii}$$
 
It can be shown that the total sample variance equals $\sum_{i=1}^{p} \lambda_{i}$. see page 433

 $\square$
```
**Geometrical Approach**

We get a better feel for what is going on in PCA with the following geometrical argument (p.468). Let's consider a scatter plot of the sample data in p-dimensional space (one dimension for each variable measured). We want to approximate each observation as a point in an r-dimensional hyperplane (nb. r < p) and we want this approximation to be as accurate as possible.

(ref:PCA) The $r=2$ dimensional plane that approximates the scatter plot by minimising the sum of square deveiations. Reproduced from [], supplement 8A$

```{r PrincipalComponents,echo=FALSE, fig.cap='(ref:PCA)',fig.align='center',dpi=20}
knitr::include_graphics("Analysis/Images/PrincipalComponents.png")
```

Algebraically the gemetric argument starts with some definitions:

```{definition, name="r-dimensional hyperplane"}

All points $\underset{(p \times 1)}{x}$ lying in an r-dimensional hyperplane through the orogin satisfy:
$$x=b_{1}\underset{(p \times 1)}{u_{1}}+b_{2}\underset{(p \times 2)}{u_{2}}+ ...+b_{r}\underset{(p \times 1)}{u_{r}}=\underset{(p \times p)}{U}\underset{(p \times 1)}{b}$$

for some b.

All points $\underset{(p \times 1)}{x}$ lying in an r-dimensional hyperplane through a point $\underset{(p \times 1)}{a}$ satify the equation:

$$x=a+Ub$$

$\square$
```


```{definition, name="geometric definition of objective function in PCA"}

Lets denote the approximtion to the jth observation $$\widehat{x_{j}}=a+Ub_{j}$$

Our objective in PCA is to find the matrix/hyperplane $U$ which minimises the squared distance $d_{j}$ between observations and their approximation. We seek to minimise:

$$\sum_{j=1}^{n}d_{j}^2=\sum_{j=1}(x_{j}-a-Ub_{j})^´(x_{j}-a-Ub_{j})$$

where $d_{j}:=x_{j}-a-Ub_{j}$ and $\sum_{j=1}^{n} b_{j}:=0$.

$\square$
```


Applying the previous minimisation lemma for the error sum of squares, we can now make the following geometrical argument. Is states that the best approximating r-dimensional plane is spanned by the first r-eigenvectors of S. Each p-dimensional data point is approximated by its projections onto these eigenvectors.

```{proposition, name="minimising the sum of squared distances in PCA"}

\begin{align}

\sum_{j=1}^{n}d_{j}^2 &= \sum_{j=1}(x_{j}-a-Ub_{j})^´(x_{j}-a-Ub_{j})\\
 &=\sum_{j=1}(x_{j}-\overline{x}-Ub_{j}+\overline{x}-a)^´(x_{j}-\overline{x}-Ub_{j}+\overline{x}-a)\\
 &=\sum_{j=1}(x_{j}-\overline{x}-Ub_{j})^´(x_{j}-\overline{x}-Ub_{j})+n(\overline{x}-a)^´(\overline{x}-a)\\
 &\geq \sum_{j=1}(x_{j}-\overline{x}-\widehat{E_{r}}\widehat{E_{r}}^´(x_{j}-\overline{x}))^´(x_{j}-\overline{x}-\widehat{E_{r}}\widehat{E_{r}}^´(x_{j}-\overline{x}))
\end{align}

where the inequality follows from the previous minimisation lemma. Minimisation occurs where $a=\overline{x}$ and $Ub_{j}=\widehat{E_{r}}\widehat{E_{r}}^´(x_{j}-\overline{x}))$

For proof see ([])
$\square$
```

In PCA the value of the projection of an observation onto the i-th eigenvector is given a special name, "the ith principal component" (see page 432):

 ```{definition, PrinComp, name="The i-th principal component"}
 
Let $\underset{(p \times p)}{\Sigma}$ be the covariance matrix associated with random vector $Z=[Z_{1},..,Z_{p}]$. Let $\Sigma$ have the eigenvalue, eigenvector pairs $(\lambda_{1},\underset{(p \times 1)}{e_{1}}),....,(\lambda_{p},\underset{(p \times 1)}{e_{p}})$ where the eigenvectors are ordered from smallest to largest. Then the ith principal component is given by:

$$Y_{i}=e_{i}^´Z$$

Furthermore this means that:

\begin{align}
Var(Y_{i})&= e_{i}^´\Sigma e_{i} = \lambda_{i} \\
Cov(Y_{i},Y_{k})&=e_{i}^´\Sigma e_{k}=0
\end{align}

$\square$
 
```

The contribution of the ith principal component to the total sample variance is
$\lambda_{i}$. 

