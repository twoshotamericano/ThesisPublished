# (PART) Theory {-}

# Multivariate Analysis {#Theory}

The following sections on Multivariate Analysis are drawn from @johnson2014applied. The [Key Elements Section](#Analysis1) below draws heavily from Chapters 2 and 3. 

##Key Elements of Multivariate Statistics {#Analysis1}

Multivariate statistics is concerned with the properties of multi-dimensional random vectors. We study the properties of the p-dimensional Random Vector $\underset{(p \times 1)}{Z}$, by capturing repeated measurements. For ease of study we arrange these n observations into an n by p data matrix (see @johnson2014applied, page 49):

```{definition, name="Data Matrix"}
$$\underset{(n \times p)}{X}=\begin{bmatrix}\underset{(1 \times p)}{X_{1}^{'}}\\\underset{(1 \times p)}{X_{2}^{'}}\\....\\\underset{(1 \times p)}{X_{n}^{'}}\end{bmatrix}=
\begin{bmatrix}X_{11} & X_{12} & ...& X_{1p}\\X_{21} & X_{22} &... & X_{2p}\\....&....&....&....\\X_{11} & X_{12} &....&X_{np}\end{bmatrix}$$

$\square$

```

A key simplifying assumption in Multivariate Analysis is that our observations constitute a Random Sample. The p-components within any observation vector $X_{i}$ are allowed to be correlated but the components in different observations must be independent. In terms of the data matrix, only components in the same row have non-zero correlation values. Furthermore we assume that the joint distribution function $f(.)$ is the same for any observation vector $X_{i}$. In symbols:

```{definition, name="Random Sample"}

\begin{equation} 
\begin{split}
f(X_{1},X_{2},...,X_{n}) & =f_{1}(X_{1})f_{2}(X_{2})..f_{n}(X_{n})\\
 & =f(X_{1})f(X_{2})..f(X_{n})
\end{split}
\end{equation} 

$\square$

```

In Multivariate Analysis, we study the properties of the sample using three fundamental objects:

```{definition, name="Sample Mean, Sample Covariance and Sample Correlation "}

The Sample Mean $\overline{X}$, Sample Covariance Matrix $S$ and Sample Correlation Matrix $R$ are defined as follows:

\begin{align}

\underset{(p \times 1)}{\overline{X}}&=\frac{\underset{(p \times n)}{X^{´}}\underset{(n \times 1)}{1}}{n}\\\\

\underset{(p \times p)}{S}&=\frac{X^{´}(I-\frac{11^{´}}{n})X}{n-1}\\\\

\underset{(p \times p)}{R}&=D^{-\frac{1}{2}}SD^{-\frac{1}{2}}

\end{align}

where $D^{-\frac{1}{2}}$ is the diagonal matrix with $D^{-\frac{1}{2}}_{ii}=\frac{1}{\sqrt{S_{ii}}}$

$\square$

```


Each of these objects has fundamental importance due to capturing different geometrical properties of the sample (see @johnson2014applied, Section 2.2, page 49). 

```{proposition, name="Geometrical Properties of the Sample"}

We start by considering the sample data matrix as p measurement vectors $Y_{i}$ in an n-dimensional space:

$$\underset{(n \times p)}{X}=
\begin{bmatrix}X_{11} & X_{12} & ...& X_{1p}\\X_{21} & X_{22} &... & X_{2p}\\....&....&....&....\\X_{11} & X_{12} &....&X_{np}\end{bmatrix}=\begin{bmatrix}\underset{(n \times 1)}{Y_{1}} & \underset{(n \times 1)}{Y_{2}} & ...& \underset{(n \times 1)}{Y_{p}}\end{bmatrix}$$

We decompose each n-dimensional measurement into a component parallel and perpendicular to the unit vector, ie. $$Y_{i}=(Y_{i} \bullet \frac{1}{\sqrt{n}})\frac{1}{\sqrt{n}}+d_{i}$$ where the deviance is defined as $d_{i}=Y_{i}-Y_{i} \bullet \frac{1}{\sqrt{n}}$ and $1=[1,1,..,1]^{´}$. 
We can then make the following identifications,

\begin{align}
\overline{X}_{i}&=Y_{i} \bullet \frac{1}{\sqrt{n}}\\
S_{ii}&=\frac{\mid d_{i}\mid ^{2}}{n-1}\\
R_{ij}&=\frac{d_{i} \bullet d_{j}}{\mid d_{i} \mid \mid d_{j} \mid}\\
\end{align}

See Ref[] page 136 for further details.

$\square$

```

Similar to when dealing with univariate random variables, certain "regularities" govern the sampling distribution of $\overline{X}$ and $S$ (see @johnson2014applied, page 175, section 4.5). These results are repeated without proof below:

```{proposition, name="Large Sample Properties of Sample Mean and Covariance"}
Let $X_{1},X_{2},...,X_{n}$ be a random sample from a population with $E[X]=\mu$ and finitie, non-singular $Cov[X]=\Sigma$.

As the number of measurements $n$ increases without bound and in any case $n \gg p$:

\begin{align}
\overline{X} &\overset{Prob}{\longrightarrow} \mu \\

S &\overset{Prob}{\longrightarrow} \Sigma \\

\sqrt{n}(\overline{X}-\mu) &\overset{Dist}{\longrightarrow} \mathcal{N}_{p}(\mu,\Sigma) \\

n(\overline{X}-\mu)^{´}S^{-1}(\overline{X}-\mu) &\overset{Dist}{\longrightarrow} \chi^2(p)

\end{align}

$\square$
```

Since in the limit of large sample sizes the  sampling distribution of $\overline{X}$ is (multivariate) normally distributed, we can apply statistical results developed for Multivariate Normal Random Vectors. For Example, the Multivariate Analysis of Variance Model for comparing g Population Mean Vectors (see @johnson2014applied, chapter 6, page 301, chapter 6)


## Linear Predictors

In this document, I restrict my attention to "Linear Predictors" of the form shown below. It follows that my predicted response variable will vary continuously with the explanatory variables.

```{definition, name="Linear Prediction Functions"}

Let $\underset{(r \times 1)}{Z}$ denote a r-dimensional vector of Explanatory Random Variables, $\underset{(1 \times 1)}{Y}$ a univariate "response"" Random variable, $\overline{\underset{(1 \times 1)}{Y}}$ a Linear Predictor of $Y$, $b_{0}$ a constant and $\underset{(r \times 1)}{b_{Z}}$ a r-dimensional vector of constant co-efficients then:

$$\overline{Y} =b_{0} + b_{Z}^{'}Z$$

$\square$

```


If I can transform my data so that it appears to be multivariate normally distributed, then the best predictor will actually be linear in the explanatory variables (see @johnson2014applied, page 404). Therefore this restriction is not significant in this scenario. In any case, linear models have the non-technical advantage of being easier to interpret and manipulate. We can use them to better understand our data before moving to more advanced techniques! We can measure their explanatory power by calculating the mean square prediction error and the multiple correlation coefficient (see below)


**Best Linear Predictors**

In keeping with the approach to Multivariate Analysis in @johnson2014applied, I will measure the accuracy of my linear prediction models using a mean square error criterion. That is, the technically best prediction model will have the lowest value for the Mean Square Prediction Error.

I can now define my prediction problem as follows:

```{definition, MSE, name="Prediction Problem"}
Let $Y$,$Z_{1}$,$Z_{2}$,...,$Z_{r}$ represent univariate random variables respectively. We assume these variables are generated from a joint distribution with population mean $\underset{(r+1) \times 1}{\mu}$ and population covariance matrix $\underset{(r+1) \times (r+1)}{\Sigma}$ which we assume to have full rank. 

We denote the linear predictor random variable $\overline{Y}$, and the prediction error random variable $\epsilon$. We define these quantities as follows:

\begin{align}
\overline{Y}&=b_{0} + b_{Z}^{'}Z\\
\epsilon&=Y-\overline{Y}
\end{align}

The objective of my linear prediction problem is to minimise the Mean Square Prediction Error (MSE) $$MSE=E(\epsilon)^2$$ by varying the values of parameters $b_{0}$ and $b_{Z}$.

$\square$
```

As will be shown later an equivalent objective function is to maximise the population multiple correlation coefficient.

```{definition, rho, name="Population Multiple Correlation Coefficient"}
If we partition the mean vector and covariance matrix of the response and explanatory variables as follows:

$$\mu = \begin{bmatrix} \underset{(1 \times 1)}{\mu_{Y}}\\
\underset{(r \times 1)}{\mu_{Z}}
\end{bmatrix}$$

$$\Sigma = \begin{bmatrix} \underset{(1 \times 1)}{\sigma_{YY}} \underset{(1 \times r)}{\sigma_{YZ}}\\
\underset{(r \times 1)}{\sigma_{ZY}} \underset{(r \times r)}{\sigma_{ZZ}}
\end{bmatrix}$$

We can define the Population Multiple Correlation Coefficient ($\rho_{Y(Z)}$):

$$\rho_{Y(Z)} := \sqrt{\frac{(\sigma_{ZY}^{´}\sigma_{ZZ}^{-1}\sigma_{ZY})}{\sigma_{YY}}}$$

$\square$
```

**Understanding Best Linear Predictors**

The following theoretical arguments reinforce our understanding of the meaning of $\rho_{Y(Z)}$. They establish $\rho_{Y(Z)}^2$ as an upper limit on the correlation between Y and its linear predictor $\overline{Y}$. They establish the interpretion of $\rho_{Y(Z)}^2$ as the power of the best linear estimator to explain the variation of Y. They establish that the best linear predictor has maximum correlation with Y ie. $\rho_{Y(Z)}^2$. (For proofs see @johnson2014applied, Section 7.8, page 401)

```{definition, name="Cauchy Schwarz inequality"}
Given two vectors $\underset{(p \times 1)}{b}$ and $\underset{(p \times 1)}{d}$, their scalar product is bounded from above $$(b^{´}d)^2 \leqslant (b^{´}b)(d^{´}d)$$

$\square$
```

```{definition, name="Extended Cauchy Schwarz inequality"}
The Extended Cauchy Schwarz inequality, with positive definite matrix $\underset{(p \times p)}{B}$, is $$(b^{´}d)^2 \leqslant (b^{´}Bb)(d^{´}B^{-1}d)$$
$\square$
```

```{proposition, upper, name="Upper Bound on Correlation between Response Variable and Linear Predictor"}

If we set $d=\underset{(r \times 1)}{\sigma_{ZY}}$, $B=\underset{(r \times 1)}{\sigma_{ZZ}}$ and $b=b_{Z}$ in the extended Cauchy Schwarz Inequality, we obtain the bound $$(b_{Z}^{´}\sigma_{ZY})^2 \leqslant (b_{Z}^{´}\sigma_{ZZ}b_{Z})(\sigma_{ZY}^{´}\sigma_{ZZ}^{-1}\sigma_{ZY})$$.

Identifying $Cov(b_{0}+b_{Z}^{´}Z,Y)=Cov(b_{Z}^{´}Z,Y)=b_{Z}^{´}Cov(Z,Y)=b_{Z}^{´}\sigma_{ZY}$, we obtain: $$[Cov(b_{0}+b_{Z}^{´}Z,Y)]^2 \leqslant (b_{Z}^{´}\sigma_{ZZ}b)(\sigma_{ZY}^{´}\sigma_{ZZ}^{-1}\sigma_{ZY})$$. 

Or equivalently for the correlation $$[Corr(b_{0}+b_{Z}^{´}Z,Y)]^2 \leqslant \frac{(\sigma_{ZY}^{´}\sigma_{ZZ}^{-1}\sigma_{ZY})}{\sigma_{YY}}=\rho_{Y(Z)}^2$$

$\square$
```

To reinforce our understanding of the meaning of $\rho_{Y(Z)}$ and its connection with the MSE, lets establish matrix identities in terms of $\mu$ and $\Sigma$ for the best linear predictor of $Y$ given $Z$:

```{proposition, name="Matrix identity for minimising the Mean Square Prediction Error"}


The Mean Square Error can be decomposed as follows:

\begin{equation} 
\begin{split}
\mathrm{MSE}(b_{0},b_{z}) & =\mathrm{E}(Y-b_{0}-b_{z}^{´}Z)^2\\
 & =\mathrm{E}(Y-b_{0}-b_{z}^{´}Z + (\mu_{Y}-b_{z}^{´}\mu_{Z}) -(\mu_{Y}-b_{z}^{´}\mu_{Z}))^2\\
 & =\mathrm{E}(Y-\mu_{Y} -(b_{z}^{´}Z-b_{z}^{´}\mu_{Z})+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z}))^2\\
 & =\mathrm{E}(Y-\mu_{Y})^2+\mathrm{E}(b_{z}^{´}(Z-\mu_{Z}))^2+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z})^2 -2\mathrm{E}(b_{z}^{´}(Z-\mu_{Z})(Y-\mu_{Y}))\\
 & =\sigma_{YY}+b_{z}^{´}\mathrm{E}((Z-\mu_{Z})(Z-\mu_{Z})^{´})b_{z}+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z})^2 -2b_{z}^{´}\mathrm{E}((Z-\mu_{Z})(Y-\mu_{Y}))\\
 & = \sigma_{YY} + b_{z}^{´}\sigma_{ZZ}b_{z}+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z})^2-2b_{z}^{´}\sigma_{ZY}\\
 & = \sigma_{YY} + b_{z}^{´}\sigma_{ZZ}b_{z}+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z})^2-2b_{z}^{´}\sigma_{ZY}+\sigma_{ZY}^{´}\Sigma_{ZZ}^{-1}\sigma_{ZY}-\sigma_{ZY}^{´}\Sigma_{ZZ}^{-1}\sigma_{ZY}\\
 &= \sigma_{YY}-\sigma_{ZY}^{´}\Sigma_{ZZ}^{-1}\sigma_{ZY}+(\mu_{Y}-b_{0}-b_{z}^{´}\mu_{Z})^2+(b_{Z}-\Sigma_{ZZ}^{-1}\sigma_{ZY})^{´}\Sigma_{ZZ}(b_{Z}-\Sigma_{ZZ}^{-1}\sigma_{ZY})
\end{split}
\end{equation} 

Minimisation is achieved by choosing the parameter values:

\begin{align} 
b_{Z} &= \Sigma_{ZZ}^{-1}\sigma_{ZY}\\
b_{0} &= \mu_{Y}-b_{Z}^{´}\mu_{Z}
\end{align} 

Whereby the minimal value of $MSE$ becomes:

$$MSE_{MIN}=\sigma_{YY}-\sigma_{ZY}^{´}\sigma_{ZZ}\sigma_{ZY}$$

$\square$
```

The link with the Multiple Correlation Coefficient is established as follows:

```{proposition, link, name="Link between Mean Squared Prediction Error and Multiple Correlation Coefficient"}

We can re-arrange the formula for the minimum mean squared prediction error as follows:

\begin{align}
MSE_{min}&=\sigma_{YY}-\sigma_{ZY}^{´}\sigma_{ZZ}\sigma_{ZY}\\
&=\sigma_{YY}-\sigma_{YY}[\frac{\sigma_{ZY}^{´}\sigma_{ZZ}\sigma_{ZY}}{\sigma_{YY}}]\\
&=\sigma_{YY}[1-\rho_{Y(Z)}^2]
\end{align}

This means that the Population Multiple Correlation Coefficient measures the "power" of the Best Linear Predictor to predict Y. If $\rho_{Y(Z)}=0$ then

$$MSE=\sigma_{YY}$$

The linear predictor has explained none of the variation in the Response Variable.
$\square$
```

Furthermore: 


```{proposition, name="Maximising Correlation between Response Variable and Linear Predictor"}

The best linear predictor of $Y$ minimises the Mean Square Prediction Error and maximises the correlation coefficient with $Y$. In fact these two quantities are directly related.

We see this by setting in the extended Cauchy Schwarz Inequality $$b_{Z}= \Sigma_{ZZ}^{-1}\sigma_{ZY}$$
$\square$
```
